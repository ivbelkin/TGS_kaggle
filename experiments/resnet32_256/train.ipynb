{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "from code.models import basicunet, resnetunet\n",
    "from code.datasets import TGSAugDataset\n",
    "from code.configs import *\n",
    "from code.train import *\n",
    "from code.losses import FocalRobustLoss\n",
    "from code.metrics import *\n",
    "from code.augmentations import *\n",
    "from code.utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "from code.inference import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augment(image, mask):\n",
    "    if np.random.rand() < 0.5:\n",
    "        image, mask = do_horizontal_flip2(image, mask)\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        if np.random.rand() < 0.5:\n",
    "            image, mask = do_elastic_transform2(image, mask, grid=10,\n",
    "                                                distort=np.random.uniform(0, 0.15))\n",
    "        if np.random.rand() < 0.5:\n",
    "            image, mask = do_random_shift_scale_crop_pad2(image, mask, 0.2)\n",
    "        if np.random.rand() < 0.5:\n",
    "            angle = np.random.uniform(0, 15)\n",
    "            scale = compute_scale_from_angle(angle * np.pi / 180)\n",
    "            image, mask = do_shift_scale_rotate2(image, mask, dx=0, dy=0, scale=scale,\n",
    "                                                 angle=angle)\n",
    "        if np.random.rand() < 0.5:\n",
    "            image, mask = do_random_perspective2(image, mask, 0.3)\n",
    "    else:\n",
    "        c = np.random.choice(4)\n",
    "        if c == 0:\n",
    "            image, mask = do_elastic_transform2(image, mask, grid=10,\n",
    "                                                distort=np.random.uniform(0, 0.15))\n",
    "        elif c == 1:\n",
    "            image, mask = do_random_shift_scale_crop_pad2(image, mask, 0.2)\n",
    "        elif c == 2:\n",
    "            angle = np.random.uniform(0, 10)\n",
    "            scale = compute_scale_from_angle(angle * np.pi / 180)\n",
    "            image, mask = do_shift_scale_rotate2(image, mask, dx=0, dy=0, scale=scale,\n",
    "                                                 angle=angle)\n",
    "        elif c == 3:\n",
    "            image, mask = do_random_perspective2(image, mask, 0.3)\n",
    "            \n",
    "    if np.random.rand() < 0.3:\n",
    "        c = np.random.choice(3)\n",
    "        if c == 0:\n",
    "            image = do_brightness_shift(image, np.random.uniform(-0.1, 0.1))  # 0.05\n",
    "        elif c == 1:\n",
    "            image = do_brightness_multiply(image, np.random.uniform(1 - 0.08, 1 + 0.08))  # 0.05\n",
    "        elif c == 2:\n",
    "            image = do_gamma(image, np.random.uniform(1 - 0.08, 1 + 0.08))  # 0.05\n",
    "    \n",
    "    image, mask = do_resize2(image, mask, 202, 202)\n",
    "    image, mask = do_center_pad_to_factor2(image, mask, factor=64)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def flip_augment(image, mask):\n",
    "    if np.random.rand() < 0.5:\n",
    "        image, mask = do_horizontal_flip2(image, mask)\n",
    "    image, mask = do_resize2(image, mask, 202, 202)\n",
    "    image, mask = do_center_pad_to_factor2(image, mask, factor=64)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def test_augment(image, mask):\n",
    "    image, mask = do_resize2(image, mask, 202, 202)\n",
    "    image, mask = do_center_pad_to_factor2(image, mask, factor=64)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37951daed8f1435994edcc1282b66067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3136), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1c0f77204842a89d98b8c7b6622714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=784), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = TGSAugDataset(augmenter=train_augment, path=os.path.join(PATH_TO_SALT_CV, \"fold-5/train\"), \n",
    "                         path_to_depths=PATH_TO_DEPTHS, progress_bar=True)\n",
    "valid_ds = TGSAugDataset(augmenter=test_augment, path=os.path.join(PATH_TO_SALT_CV, \"fold-5/valid\"), \n",
    "                         path_to_depths=PATH_TO_DEPTHS, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=16, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetResNet34Wrapped(resnetunet.UNetResNet34):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def __call__(self, image, **kwargs):\n",
    "        logits = super().__call__(image)\n",
    "        return {\"logits\": logits[:,0]}\n",
    "\n",
    "\n",
    "class BasicUNetWrapped(basicunet.BasicUNet):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def __call__(self, image, **kwargs):\n",
    "        image = image.unsqueeze(1)\n",
    "        logits = super().__call__(image)\n",
    "        return {\"logits\": logits[:,0]}\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = FocalRobustLoss(gamma=2.0, alpha=0.05)\n",
    "        \n",
    "    def forward(self, logits, mask, **kwargs):\n",
    "        logits = logits[:,26:-26,26:-26].contiguous()\n",
    "        mask = mask[:,26:-26,26:-26].contiguous()\n",
    "        loss = self.loss_fn(logits, mask)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class LovaszLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = lovasz_elu\n",
    "        \n",
    "    def forward(self, logits, mask, **kwargs):\n",
    "        logits = logits[:,26:-26,26:-26].contiguous()\n",
    "        mask = mask[:,26:-26,26:-26].contiguous()\n",
    "        loss = self.loss_fn(logits, mask)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "def lossWrapped(logits, mask, **kwargs):\n",
    "    logits = logits[:,27:-27,27:-27].contiguous()\n",
    "    mask = mask[:,27:-27,27:-27].contiguous()\n",
    "    loss = lovasz_elu(logits, mask)\n",
    "    return loss.item()\n",
    "    \n",
    "    \n",
    "def sigmoid(logits):\n",
    "    return 1. / (1 + np.exp(-logits))\n",
    "    \n",
    "\n",
    "def meanAPWrapped(logits, mask, treashold, **kwargs):\n",
    "    logits_cpu = logits[:,27:-27,27:-27].cpu().detach().numpy()\n",
    "    mask_cpu = mask[:,27:-27,27:-27].cpu().detach().numpy()\n",
    "    return meanAP2d(sigmoid(logits_cpu), mask_cpu, treashold)\n",
    "\n",
    "\n",
    "def meanIoUWrapped(logits, mask, treashold, **kwargs):\n",
    "    logits_cpu = logits[:,27:-27,27:-27].cpu().detach().numpy()\n",
    "    mask_cpu = mask[:,27:-27,27:-27].cpu().detach().numpy()\n",
    "    return meanIoU2d(sigmoid(logits_cpu), mask_cpu, treashold)\n",
    "\n",
    "###################################################################\n",
    "\n",
    "def meanSoftIoUWrapped(logits, mask, **kwargs):\n",
    "    logits_cpu = logits[:,13:-14,13:-14].cpu().detach().numpy()\n",
    "    mask_cpu = mask[:,13:-14,13:-14].cpu().detach().numpy()\n",
    "    return meanSoftIoU2d(sigmoid(logits_cpu), mask_cpu)\n",
    "\n",
    "\n",
    "def meanAccuracyWrapped(logits, mask, **kwargs):\n",
    "    logits_cpu = logits[:,13:-14,13:-14].cpu().detach().numpy()\n",
    "    mask_cpu = mask[:,13:-14,13:-14].cpu().detach().numpy()\n",
    "    return meanAccuracy2d(sigmoid(logits_cpu), mask_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_elu(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_elu_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_elu_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_sigmoid(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz sigmoid loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_sigmoid_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * signs)\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]    \n",
    "    grad = lovasz_grad(gt_sorted)    \n",
    "    loss = torch.dot(F.relu(errors_sorted), grad)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_elu_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    #if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "    #    return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]    \n",
    "    grad = lovasz_grad(gt_sorted)    \n",
    "    loss = torch.dot(F.elu(errors_sorted) + 1, Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_sigmoid_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz sigmoid loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    errors = (labels - F.sigmoid(logits)).abs()\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]    \n",
    "    grad = lovasz_grad(gt_sorted)    \n",
    "    loss = torch.dot(errors_sorted, Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5422145"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = UNetResNet34Wrapped(device)\n",
    "model.load_pretrain(PATH_TO_RESNET34)\n",
    "sum([p.nelement() for p in model.parameters()]) - sum([p.nelement() for p in model.resnet.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "loss_fn = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"log5.txt\", [\"epoch\", \"train_loss\", \"valid_loss\",\n",
    "                                \"train_mAP_0.5\",\n",
    "                                \"train_mIoU_0.5\",\n",
    "                                \"valid_mAP_0.5\", \n",
    "                                \"valid_mIoU_0.5\"], \n",
    "                   log_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_cp = BestLastCheckpointer(\"mAP5\")\n",
    "mIoU_cp = BestLastCheckpointer(\"mIoU5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(model.resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.resetClock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # 10\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    loss = train_epoch_fn(model, train_dl, optim, loss_fn, verbose=1, loss_file=\"focal_loss.txt\")\n",
    "    \n",
    "    train_metrics = eval_fn(model, train_dl, \n",
    "            {\n",
    "                \"train_loss\": lossWrapped, \n",
    "                \"train_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"train_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "            verbose=1)\n",
    "    valid_metrics = eval_fn(model, valid_dl, \n",
    "            {\n",
    "                \"valid_loss\": lossWrapped, \n",
    "                \"valid_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"valid_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "                            verbose=1)\n",
    "    \n",
    "    logger.write(epoch=epoch, **train_metrics, **valid_metrics)\n",
    "    \n",
    "    mAP_cp.update(-valid_metrics[\"valid_mAP_0.5\"], \n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    mIoU_cp.update(-valid_metrics[\"valid_mIoU_0.5\"],\n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # 20\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    loss = train_epoch_fn(model, train_dl, optim, loss_fn, verbose=1, loss_file=\"focal_loss.txt\")\n",
    "    \n",
    "    train_metrics = eval_fn(model, train_dl, \n",
    "            {\n",
    "                \"train_loss\": lossWrapped, \n",
    "                \"train_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"train_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "            verbose=1)\n",
    "    valid_metrics = eval_fn(model, valid_dl, \n",
    "            {\n",
    "                \"valid_loss\": lossWrapped, \n",
    "                \"valid_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"valid_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "                            verbose=1)\n",
    "    \n",
    "    logger.write(epoch=epoch, **train_metrics, **valid_metrics)\n",
    "    \n",
    "    mAP_cp.update(-valid_metrics[\"valid_mAP_0.5\"], \n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    mIoU_cp.update(-valid_metrics[\"valid_mIoU_0.5\"],\n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "loss_fn = LovaszLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(60):  # 60\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    loss = train_epoch_fn(model, train_dl, optim, loss_fn, verbose=1, loss_file=\"lovasz_loss.txt\")\n",
    "    \n",
    "    train_metrics = eval_fn(model, train_dl, \n",
    "            {\n",
    "                \"train_loss\": lossWrapped, \n",
    "                \"train_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"train_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "            verbose=1)\n",
    "    valid_metrics = eval_fn(model, valid_dl, \n",
    "            {\n",
    "                \"valid_loss\": lossWrapped, \n",
    "                \"valid_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"valid_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "                            verbose=1)\n",
    "    \n",
    "    logger.write(epoch=epoch, **train_metrics, **valid_metrics)\n",
    "    \n",
    "    mAP_cp.update(-valid_metrics[\"valid_mAP_0.5\"], \n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    mIoU_cp.update(-valid_metrics[\"valid_mIoU_0.5\"],\n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_cp.load(\"best\", model=model, optim=optim)\n",
    "set_learning_rate(optim, 0.00002)\n",
    "freeze(model, include=(torch.nn.BatchNorm2d,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # 30\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    loss = train_epoch_fn(model, train_dl, optim, loss_fn, verbose=1, loss_file=\"lovasz_loss.txt\")\n",
    "    \n",
    "    train_metrics = eval_fn(model, train_dl, \n",
    "            {\n",
    "                \"train_loss\": lossWrapped, \n",
    "                \"train_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"train_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "            verbose=1)\n",
    "    valid_metrics = eval_fn(model, valid_dl, \n",
    "            {\n",
    "                \"valid_loss\": lossWrapped, \n",
    "                \"valid_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"valid_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "                            verbose=1)\n",
    "    \n",
    "    logger.write(epoch=epoch, **train_metrics, **valid_metrics)\n",
    "    \n",
    "    mAP_cp.update(-valid_metrics[\"valid_mAP_0.5\"], \n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    mIoU_cp.update(-valid_metrics[\"valid_mIoU_0.5\"],\n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.augmenter = flip_augment\n",
    "mAP_cp.load(\"best\", model=model, optim=optim)\n",
    "set_learning_rate(optim, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f919c7638f45c8b5973316d279c5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=392), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d8badf599041e08dd9c35e03f655d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=392), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(10):  # 10\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    loss = train_epoch_fn(model, train_dl, optim, loss_fn, verbose=1, loss_file=\"lovasz_loss.txt\")\n",
    "    \n",
    "    train_metrics = eval_fn(model, train_dl, \n",
    "            {\n",
    "                \"train_loss\": lossWrapped, \n",
    "                \"train_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"train_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "            verbose=1)\n",
    "    valid_metrics = eval_fn(model, valid_dl, \n",
    "            {\n",
    "                \"valid_loss\": lossWrapped, \n",
    "                \"valid_mAP_0.5\": lambda logits, mask, **kwargs: meanAPWrapped(logits, mask, 0.5), \n",
    "                \"valid_mIoU_0.5\": lambda logits, mask, **kwargs: meanIoUWrapped(logits, mask, 0.5), \n",
    "            }, \n",
    "                            verbose=1)\n",
    "    \n",
    "    logger.write(epoch=epoch, **train_metrics, **valid_metrics)\n",
    "    \n",
    "    mAP_cp.update(-valid_metrics[\"valid_mAP_0.5\"], \n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    mIoU_cp.update(-valid_metrics[\"valid_mIoU_0.5\"],\n",
    "                    model=model, optim=optim, epoch=epoch)\n",
    "    \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo poweroff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
